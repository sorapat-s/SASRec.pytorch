{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB7Vq_7Enq1b"
   },
   "source": [
    "## Count Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm3RcDeTnq1c",
    "outputId": "65a26a0a-556a-412a-aa55-4571968bff1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Interactions: 999611\n"
     ]
    }
   ],
   "source": [
    "with open(\"ml-1m.txt\", \"r\") as file:\n",
    "    print(f\"Total Interactions: {sum(1 for line in file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuB8S8NLnq1e",
    "outputId": "a02ca927-a864-461a-933a-46e914776083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Interactions: 1000209\n"
     ]
    }
   ],
   "source": [
    "with open(\"ratings.dat\",\"r\") as f:\n",
    "    print(f\"Total Interactions: {sum(1 for line in f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcnuDaE-nq1e"
   },
   "source": [
    "## Count user and item frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bTVeXBCnq1f"
   },
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# # Load data\n",
    "# data = []\n",
    "# with open(\"Steam.txt\", \"r\") as file:\n",
    "#     for line in file:\n",
    "#         user, item = line.strip().split()\n",
    "#         data.append((user, item))\n",
    "\n",
    "# # Count user and item frequencies\n",
    "# user_freq = defaultdict(int)\n",
    "# item_freq = defaultdict(int)\n",
    "# for user, item in data:\n",
    "#     user_freq[user] += 1\n",
    "#     item_freq[item] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9qjgPFJnq1g"
   },
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jboq_x9lnq1g"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # โหลดข้อมูล\n",
    "# file_path = \"ml-1m.txt\"\n",
    "# df = pd.read_csv(file_path, sep=\" \", header=None, names=[\"user_id\", \"item_id\"])\n",
    "\n",
    "# # กรองเฉพาะ user_id ที่มี interaction เกิน 5\n",
    "# user_interactions = df['user_id'].value_counts()\n",
    "# valid_user_ids = user_interactions[user_interactions > 5].index\n",
    "# filtered_df = df[df['user_id'].isin(valid_user_ids)]\n",
    "\n",
    "# # สุ่ม user_id แบบสุ่มลำดับ\n",
    "# sample_user_ids = filtered_df['user_id'].drop_duplicates().sample(frac=1, random_state=42)\n",
    "# selected_records = pd.DataFrame(columns=filtered_df.columns)\n",
    "\n",
    "# # เลือกข้อมูลโดยไม่เกิน 100,000 records\n",
    "# for user_id in sample_user_ids:\n",
    "#     user_data = filtered_df[filtered_df['user_id'] == user_id]\n",
    "#     if len(selected_records) + len(user_data) > 100000:\n",
    "#         break\n",
    "#     selected_records = pd.concat([selected_records, user_data])\n",
    "\n",
    "# # เก็บลำดับเดิม\n",
    "# selected_records = selected_records.sort_index()\n",
    "\n",
    "# # สร้าง mapping ใหม่ของ user_id เป็น 1, 2, 3, ..., n\n",
    "# unique_user_ids = selected_records['user_id'].drop_duplicates().reset_index(drop=True)\n",
    "# user_id_mapping = {old_id: new_id + 1 for new_id, old_id in enumerate(unique_user_ids)}\n",
    "# selected_records['user_id'] = selected_records['user_id'].map(user_id_mapping)\n",
    "\n",
    "# # บันทึกไฟล์ใหม่\n",
    "# selected_records.to_csv(\"ml-1m_random.txt\", sep=\" \", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14219,
     "status": "ok",
     "timestamp": 1732086282589,
     "user": {
      "displayName": "Sorapat Samsombudsagoon",
      "userId": "06943787507769540811"
     },
     "user_tz": -420
    },
    "id": "VLjTl_WDnq1h",
    "outputId": "89435a82-0bfd-4867-e100-77cac8eec70d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4982, 884, 5222, 1116, 1798, 3772, 4024, 3349, 4634, 2649, 427, 4253, 4638, 1604, 5932, 4753, 5954, 1359, 3425, 1073, 2913, 402, 790, 4856, 5429, 3566, 5926, 910, 1145, 4964, 3301, 5843, 2117, 3893, 771, 2972, 720, 5191, 3142, 1991, 1739, 1972, 2299, 2921, 5175, 1607, 2835, 3654, 1559, 5329, 4425, 1072, 4774, 5395, 5305, 3852, 5874, 3681, 2353, 1542, 219, 1860, 5054, 4193, 390, 1485, 5113, 3499, 4316, 1428, 1827, 1732, 3273, 4049, 2222, 801, 4861, 5677, 56, 1866, 3969, 1903, 4608, 1497, 3792, 2087, 5651, 717, 1067, 1484, 2426, 2946, 3492, 974, 1293, 4631, 2066, 130, 919, 13, 1332, 1624, 1473, 5115, 5239, 824, 4465, 1012, 3131, 5829, 4131, 4732, 743, 1163, 3262, 4236, 2231, 1260, 3073, 5501, 2900, 1666, 1000, 1741, 4817, 4116, 2845, 5279, 3192, 870, 1312, 457, 2192, 3545, 590, 4747, 4038, 3867, 3463, 1121, 2251, 1958, 3688, 2161, 2198, 2023, 1611, 3758, 4613, 1834, 5613, 1642, 1889, 3584, 1, 3487, 2331, 2536, 1488, 2896, 5255, 5275, 2108, 2440, 192, 5806, 4334, 3189, 72, 1687, 2226, 1465, 3708, 1410, 4042, 1495, 951, 6011, 2006, 4394, 2059, 2326, 5644, 4190, 1646, 584, 1747, 4870, 1657, 3652, 3746, 687, 4070, 5297, 3956, 4072, 5032, 1580, 451, 1112, 4832, 3147, 2403, 4653, 2914, 4449, 924, 1393, 1168, 4388, 2516, 4144, 3868, 4867, 4214, 1193, 2701, 1046, 969, 1927, 4378, 2851, 2784, 2901, 4991, 1416, 5349, 4702, 2941, 4587, 4133, 593, 2660, 139, 1987, 5343, 5857, 3137, 5878, 5948, 5170, 4719, 5971, 420, 3077, 644, 4286, 1852, 796, 1531, 3452, 4108, 6031, 1563, 4413, 2033, 4082, 3398, 5702, 1266, 5414, 4281, 2689, 960, 1557, 375, 1412, 2566, 4526, 2123, 73, 1378, 4865, 709, 5130, 4276, 1689, 1864, 4584, 658, 990, 3649, 2256, 1700, 5013, 4296, 596, 1451, 659, 4117, 5271, 4731, 4249, 2151, 1764, 4295, 2629, 4669, 4484, 3181, 3504, 697, 5927, 5966, 5433, 3172, 1989, 5411, 2627, 4018, 1399, 2847, 136, 5941, 2587, 1977, 3085, 1893, 2313, 4986, 1703, 3307, 2732, 5964, 3801, 1472, 1788, 1938, 5071, 4737, 894, 1148, 5529, 5452, 1522, 3731, 2267, 3321, 3920, 1897, 3062, 4987, 1848, 675, 1158, 1276, 5189, 3005, 2764, 3035, 2430, 3715, 2638, 3813, 2459, 1099, 2771, 3127, 5549, 4977, 3011, 1677, 3043, 2571, 1745, 3529, 5628, 5052, 5094, 492, 4166, 5145, 2372, 5426, 4307, 493, 667, 3344, 4652, 781, 2021, 198, 2603, 3449, 5393, 406, 3454, 1443, 1635, 4567, 1432, 1815, 3469, 5195, 3712, 516, 3322, 1215, 3486, 4422, 4420, 1382, 4662, 3817, 3954, 4200, 2755, 4140, 5209, 5709, 1355, 2839, 2152, 2958, 5835, 2557, 482, 1853, 2325, 5067, 3363, 5454, 3799, 4547, 5559, 4168, 4044, 1240, 730, 901, 2540, 708, 4616, 843, 4524, 4957, 4421, 1654, 5747, 5384, 4953, 5520, 3016, 4288, 622, 5886, 5502, 5413, 5567, 4065, 4739, 3182, 1535, 3607, 849, 4672, 4701, 3466, 2825, 2081, 2252, 5741, 4026, 1589, 4360, 4742, 5979, 4284, 2507, 5735, 5210, 8, 323, 815, 2270, 325, 3845, 3034, 1796, 5366, 5648, 1411, 5181, 2527, 2777, 4640, 1480, 272, 2818, 1288, 5911, 3418, 3328, 3959, 1774, 3576, 3790, 4010, 1383, 4300, 5418, 4998, 1549, 5435, 4406, 3174, 1108, 5080, 1449, 1835, 5509, 5743, 3595, 2866, 5748, 4717, 936, 2306, 881, 2260, 3357, 5345, 3677, 3060, 601, 5530, 4091, 1171, 4291, 180, 5983, 5036, 3740, 1006, 488, 3296, 3415, 479, 1819, 3638, 5408, 1760, 2504, 610, 4715, 2318, 2464, 747, 4578, 2235, 889, 1880, 5431, 1775, 2209, 4992, 4279, 5263, 3983, 2691, 3146, 444, 5773, 5148, 1831, 2531, 3083, 1947, 5799, 428, 929, 347, 4553, 5913, 1097, 3383, 2435, 1806, 201, 3843, 3674, 3228, 1753]\n",
      "3304\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# โหลดข้อมูลจากไฟล์ ratings.dat\n",
    "file_path = 'ml-1m-ratings.dat'  # เปลี่ยน path ตามที่เก็บไฟล์\n",
    "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "\n",
    "# อ่านไฟล์ด้วยการแยก \"::\"\n",
    "ratings_df = pd.read_csv(file_path, sep='::', names=columns, engine='python')\n",
    "\n",
    "# กรองเฉพาะ user_id ที่มี interaction > 5\n",
    "user_interactions = ratings_df['user_id'].value_counts()\n",
    "valid_user_ids = user_interactions[user_interactions > 5].index\n",
    "filtered_df = ratings_df[ratings_df['user_id'].isin(valid_user_ids)]\n",
    "\n",
    "total_interactions = 0\n",
    "selected_user = []\n",
    "while True:\n",
    "    random_user = random.choice(filtered_df['user_id'].unique())\n",
    "    while random_user in selected_user: random_user = random.choice(filtered_df['user_id'].unique())\n",
    "    selected_user.append(random_user)\n",
    "    each_interactions = filtered_df[filtered_df['user_id'] == random_user]\n",
    "    total_interactions += len(each_interactions)\n",
    "    if total_interactions > 100000:\n",
    "        break\n",
    "print(selected_user)\n",
    "\n",
    "filtered_df = filtered_df[filtered_df['user_id'].isin(selected_user)]\n",
    "\n",
    "# สุ่ม interactions โดยตรง\n",
    "sampled_interactions = filtered_df.sample(n=min(100000, len(filtered_df)), random_state=42)\n",
    "\n",
    "# จัดเรียงตาม timestamp\n",
    "selected_records = sampled_interactions.sort_values(by=['timestamp']).reset_index(drop=True)\n",
    "\n",
    "# ตรวจสอบว่า user_id ใน selected_records มี interaction ขั้นต่ำ 5\n",
    "valid_user_ids = selected_records['user_id'].value_counts()\n",
    "valid_user_ids = valid_user_ids[valid_user_ids >= 5].index\n",
    "selected_records = selected_records[selected_records['user_id'].isin(valid_user_ids)].reset_index(drop=True)\n",
    "\n",
    "# สร้าง mapping ใหม่สำหรับ user_id และ item_id\n",
    "unique_user_ids = selected_records['user_id'].drop_duplicates().reset_index(drop=True)\n",
    "user_id_mapping = {old_id: new_id + 1 for new_id, old_id in enumerate(unique_user_ids)}\n",
    "selected_records['user_id'] = selected_records['user_id'].map(user_id_mapping)\n",
    "\n",
    "unique_item_ids = selected_records['item_id'].drop_duplicates().reset_index(drop=True)\n",
    "item_id_mapping = {old_id: new_id + 1 for new_id, old_id in enumerate(unique_item_ids)}\n",
    "selected_records['item_id'] = selected_records['item_id'].map(item_id_mapping)\n",
    "\n",
    "# แบ่งข้อมูลเป็น train และ test สำหรับ NCF\n",
    "train_data = []\n",
    "test_data = []\n",
    "negative_samples = []\n",
    "\n",
    "print(max(selected_records['item_id'].unique()))\n",
    "all_items = set(range(max(selected_records['item_id'].unique())))\n",
    "\n",
    "for user_id in selected_records['user_id'].unique():\n",
    "    user_data = selected_records[selected_records['user_id'] == user_id]\n",
    "    items = user_data['item_id'].tolist()\n",
    "    ratings = user_data['rating'].tolist()\n",
    "    timestamps = user_data['timestamp'].tolist()\n",
    "\n",
    "    # ใช้รายการสุดท้ายเป็น test\n",
    "    test_data.append([user_id, items[-1], ratings[-1], timestamps[-1]])\n",
    "\n",
    "    # รายการอื่นใช้เป็น train\n",
    "    for item, rating, timestamp in zip(items[:-1], ratings[:-1], timestamps[:-1]):\n",
    "        train_data.append([user_id, item, rating, timestamp])\n",
    "\n",
    "    # สุ่ม negative samples\n",
    "    interacted_items = set(items)\n",
    "    negative_items = list(all_items - interacted_items)\n",
    "    negative_samples.append([user_id] + list(np.random.choice(negative_items, size=99, replace=False)))\n",
    "\n",
    "# สร้าง DataFrame สำหรับ train และ test\n",
    "train_df = pd.DataFrame(train_data, columns=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "test_df = pd.DataFrame(test_data, columns=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "# บันทึกไฟล์สำหรับ NCF (รวม rating และ timestamp)\n",
    "train_df.to_csv(\"ml-sample-100k.train.rating\", sep=\"\\t\", index=False, header=False)\n",
    "test_df.to_csv(\"ml-sample-100k.test.rating\", sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "# สร้าง negative samples ในฟอร์แมตที่กำหนด\n",
    "negative_samples_formatted = []\n",
    "for user_id, negatives in zip([x[0] for x in negative_samples], [x[1:] for x in negative_samples]):\n",
    "    negatives_line = f\"({user_id},{test_df[test_df['user_id'] == user_id]['item_id'].iloc[0]})\\t\" + \"\\t\".join(map(str, negatives))\n",
    "    negative_samples_formatted.append(negatives_line)\n",
    "\n",
    "# บันทึกไฟล์ ncf_test.negative\n",
    "with open(\"ml-sample-100k.test.negative\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(negative_samples_formatted))\n",
    "\n",
    "# บันทึกไฟล์สำหรับ SASRec โดยจัดเรียงตาม user_id และ timestamp\n",
    "sasrec_data = selected_records[['user_id', 'item_id', 'timestamp']]\n",
    "sasrec_data = sasrec_data.sort_values(by=['user_id', 'timestamp']).reset_index(drop=True)\n",
    "sasrec_data[['user_id', 'item_id']].to_csv(\"ml-sample-100k.txt\", sep=\" \", index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
